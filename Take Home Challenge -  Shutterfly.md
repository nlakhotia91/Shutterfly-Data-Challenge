---
typora-copy-images-to: ./cluster scores.png
---

# Take Home Challenge -  Shutterfly

## 1) Customer Segmentation

From ORDER and ONlLINE dataset, following features were derived at customer level-

- Recency: Last transaction of customer
- Cust_age: First time customer made transaction
- Frequency: Number of times customer has made transactions
- Monetary Value: Total revenue generated by customers
- Prodcat1_distinct_count: Number distinct  of productcat1 purchased

After data manipulation and data scaling, K means clustering was run on the data. To choose the number of optimal clusters, elbow graph was used (shown below).

![silhouette_score](https://github.com/nlakhotia91/Shutterfly-Data-Challenge/blob/master/silhouette_score.png)

 The elbow graph graph suggests that for cluster number 3 or more, the silhouette score remains constant almost. Hence, I decided to go build 4 clusters on the data. Below is the cluster plot and cluster scores for each variable-

![Cluster](https://github.com/nlakhotia91/Shutterfly-Data-Challenge/blob/master/Cluster.png)

![cluster scores](https://github.com/nlakhotia91/Shutterfly-Data-Challenge/blob/master/cluster%20scores.png)

The cluster scores were used to interpret the behaviour of customers it represents -

<u>Cluster 0:</u> High customer age (0.64), low online activity, low frequency in purchase, low revenue and high recency suggesting their last transaction was long time ago. This cluster represents customers who have **churned out** or in the process of churning out.

<u>Cluster 1:</u> Highest monetary value, customer age, frequency with most recent purchases (low recency corresponds to low numbers of days since last purchase). This cluster represents the most **loyal customers** who spends the most and are regular in purchase with moderate number of online events.

<u>Cluster 2:</u> Relatively new customers (customer age -0.02) with relatively low revenue, moderately high recency (suggesting high number of days passes since last transaction), low purchase frequency but very highest number of online events. These customers have lot of events online but purchase frequency is low.

Probably, giving some sort of discounts to these customers will give them a nudge it make a purchase, else they are on churners paths.

<u>Cluster 3:</u> Lowest customer age suggest they are very new customers, still having relatively high revenue generation, least number of online events. They lie in middle tier.

##  2) Recommendation Model

Before going through the final methodology, below are the bottlenecks I faced-

- No way to map ONLINE dataset with ORDER dataset because 

  1) prodcat2 in ONLINE dataset has only 3 distinct values, whereas ORDER dataset has 6 distinct values. Since prodcat1 prediction is our output so I decided not to build model using ONLINE data as it would led to data leakage.

  2) Mapping of ONLINE and ORDER dataset was not fruitful. I attempted to merge them with the assumption that one of events in the ONLINE dataset would be 'Purchase' event and hence every transaction in 'ORDER' dataset would have an 'Purchase' event from ONLINE dataset. But that was not the case. Also, my assumptions was ORDER dataset had online historical transactions.

- Since no information was given on events column of ONLINE dataset (as in numbers 1-10 represents wat?), I could not construct lifecycle of customer transaction to know what all steps customer is taking before finally making any purchase. 



In oder to build prediction of next prodcat1 for every customer, I decided to build recommendation model. 

From ORDER dataset, I build 3 different datasets-

- Prodcat2 purchase count at customer, prodcat2 level.

- Dummy purchase at customer, prodcat2 level.

- Scaled Prodcat2 purchase count at customer, prodcat2 level.

For modeling, I tried 3 different models-

**Popularity Model** (which was my Baseline Model), **Collaborative Filtering Model using Cosine Similarity** and another using **Pearson Similarity**.

In total 9 models were built- (3 datasets * 3 models)

Evolution was done using RMSE and precision-recall metrics. After evaluating all the models, I decided to go ahead with Purchase Dummy dataset using Cosine Similarity Collaborative Model.

I built the model to recommend top 15 items from prodcat2 for each customers. Once I got the recommendations, I mapped the prodcat2 to prodcat1 and took the majority count of prodcat1 (meaning prodcat1 with maximum entires in the top 15 ranks was assigned as the product category to recommend).

